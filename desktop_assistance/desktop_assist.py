# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'pyQts\desktop_assist.ui'
#
# Created by: PyQt5 UI code generator 5.15.0
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.


from PyQt5 import QtCore, QtGui, QtWidgets
#from VideoCapture import Device
import  cv2
import numpy as np

#from tensorflow.keras.models import Sequential
#from tensorflow.keras.layers import Dense, Dropout, Flatten
#from tensorflow.keras.layers import Conv2D
#from tensorflow.keras.layers import MaxPooling2D
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import os
import datetime
#import speech_recognition as sr
#from playsound import playsound
#import gtts
#from main import communicate

class Ui_Assistance(object):
    """
    def speak(self, text):
        tts = gtts.gTTS(text, lang='en')
        tts.save('hello.mp3')
        playsound('hello.mp3')
        os.remove('hello.mp3')
    """
    
    def wishMe(self):
        hour = int(datetime.datetime.now().hour)
        if hour >= 0 and hour < 12:
            self.dialogarea.setText("Good Morning Master")
            #self.speak('Good Morning Master')
    
        elif hour >= 12 and hour < 18:
            self.dialogarea.setText("Good Afternoon Master")
            #self.speak('good afternoon master')
    
        else:
            self.dialogarea.setText('good evening')
            #self.dialogarea.setText("Good Evening Master")
        #self.speak('I am your desktop assistant. How may I help you?')
    
    def open_camera(self):
        
        self.wishMe()
        
        model = Sequential()

        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))
        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.25))
        
        model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Dropout(0.25))
        
        model.add(Flatten())
        model.add(Dense(1024, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(7, activation='softmax'))
        
        
        model.load_weights('model.h5')
    # prevents openCL usage and unnecessary logging messages
        cv2.ocl.setUseOpenCL(False)
    # dictionary which assigns each label an emotion (alphabetical order)
        emotion_dict = {0: "Angry", 1: "Disgusted", 2: "Fearful", 3: "Happy", 4: "Neutral", 5: "Sad", 6: "Surprised"}
        
        # start the webcam feed
        cap = cv2.VideoCapture(0)
        emotions = []
        while True:
                # Find haar cascade to draw bounding box around face
            ret, frame = cap.read()
            if not ret:
                break
            facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)
        
            for (x, y, w, h) in faces:
                cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)
                roi_gray = gray[y:y + h, x:x + w]
                cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)
                prediction = model.predict(cropped_img)
                maxindex = int(np.argmax(prediction))
                #cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
        
                emotions.append(emotion_dict[maxindex])
                
            cv2.imshow('Video', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        
        emotion = emotions[-1]
        
        if emotion == 'Fearful':
            self.dialogarea.setText("You look fearful, you need to be calm, what is the issue?")
            
        elif emotion == 'Sad':
            self.dialogarea.setText("It seems you look sad, what's the problem?")
        elif emotion == 'Angry':
            self.dialogarea.setText("You are angry. What got you angry?")
        elif emotion == 'Surprised':
            self.dialogarea.setText("Why are you surprised, hope something good just happened?")
        elif emotion == 'Disgusted':
            self.dialogarea.setText("It feels disgusting right?, You wanna talk about it?")
        elif emotion == 'Neutral':
            self.dialogarea.setText("You look abit worried, what's on your mind?")
        elif emotion == 'Happy':
            self.dialogarea.setText("You look happy and excited, what's the catch? \nshare with me now")
    

    def enter(self):
        self.dialogarea.setText(self.dialogarea.toPlainText()+ "\n\nMaster: "+ self.lineEdit.text())
        self.resp = self.lineEdit.text()
        
        resp = communicate(self.resp)

        self.dialogarea.setText(str(self.dialogarea.toPlainText()) + resp)
        self.lineEdit.setText("")
            
    def checkface(self):
        self.open_camera
        uniq = 'boss'
        filename = '%s.jpg'%uniq
        cam = Device()
        cam.saveSnapshot('myimg/'+filename)
                
    
    def setupUi(self, Assistance):
        
        Assistance.setObjectName("Assistance")
        Assistance.resize(593, 310)
        self.btn_start = QtWidgets.QPushButton(Assistance)
        self.btn_start.setGeometry(QtCore.QRect(100, 120, 111, 61))
        font = QtGui.QFont()
        font.setFamily("Bauhaus 93")
        font.setPointSize(16)
        self.btn_start.setFont(font)
        self.btn_start.setObjectName("btn_start")
        self.btn_start.clicked.connect(self.open_camera)
        
        self.dialogarea = QtWidgets.QTextEdit(Assistance)
        self.dialogarea.setGeometry(QtCore.QRect(340, 0, 251, 281))
        self.dialogarea.setObjectName("dialogarea")
        
        self.lblback = QtWidgets.QLabel(Assistance)
        self.lblback.setGeometry(QtCore.QRect(-4, 0, 601, 311))
        self.lblback.setText("")
        self.lblback.setObjectName("lblback")
        
        self.lineEdit = QtWidgets.QLineEdit(Assistance)
        self.lineEdit.setGeometry(QtCore.QRect(340, 288, 191, 20))
        self.lineEdit.setObjectName("lineEdit")
        
        self.btn_enter = QtWidgets.QPushButton(Assistance)
        self.btn_enter.setGeometry(QtCore.QRect(540, 286, 51, 23))
        self.btn_enter.setObjectName("btn_enter")
        self.btn_enter.clicked.connect(self.enter)
        
        self.btn_sp = QtWidgets.QPushButton(Assistance)
        self.btn_sp.setGeometry(QtCore.QRect(200, 406, 51, 23))
        self.btn_sp.setObjectName("btn_sp")
        self.btn_sp.clicked.connect(self.wishMe)
        
        self.lblback.raise_()
        self.btn_start.raise_()
        self.dialogarea.raise_()
        self.lineEdit.raise_()
        self.btn_enter.raise_()
        self.btn_sp.raise_()

        self.retranslateUi(Assistance)
        QtCore.QMetaObject.connectSlotsByName(Assistance)

    def retranslateUi(self, Assistance):
        _translate = QtCore.QCoreApplication.translate
        Assistance.setWindowTitle(_translate("Assistance", "Assistant"))
        self.btn_start.setText(_translate("Assistance", "START"))
        self.btn_enter.setText(_translate("Assistance", "Enter"))
        self.btn_sp.setText(_translate("Assistance", "Speak"))


if __name__ == "__main__":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    Assistance = QtWidgets.QDialog()
    ui = Ui_Assistance()
    ui.setupUi(Assistance)
    Assistance.show()
    sys.exit(app.exec_())
